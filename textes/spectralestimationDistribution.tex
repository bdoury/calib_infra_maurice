% !TEX root = ../distCjHMSC.tex
%==============================================================
%==============================================================
\section{Non parametrical spectral estimation}
%==============================================================
Let us consider $x_{n=0:N-1}$ $N$ successive values of a real WSS bivariate process with zero-mean and spectral matrix $\Gamma(f)$. Its DFT writes
$$
 X_{k}=\frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_{n}e^{-2j\pi kn/N}
$$
It can be shown that, for large $N$, and for $k\neq 0$ and $k\neq N/2$:
$$
 X_{k}\simiid\mathcal{N}_{c}(0,\Gamma_{k})
$$
where $\Gamma_{k}=\Gamma(k/N)$ and where $\mathcal{N}_{c}$ denotes the complex circular gaussian distribution. The periodogram is defined by 
$$
 P_{k}=X_{k}X_{k}^{H}
$$
In the following section \ref{ann:peridogramproperties}, it is shown that the expectation of the periodogram $P_{k}$ is $\Gamma_{k}$, but unfortunately the dispersion around $\Gamma_{k}$ never goes to 0 when $N$ goes to infinity. The periodogram is not a consistent estimate of $\Gamma_{k}$. A fundamental way to have consistency is to smooth the periodogram (smoothed peridogram). An other approach consists to segment the data in $(2M+1)$ $50\%$-overlapping blocks and averaging the $2M+1$ periodograms, applying a window on each block. That is called Welch's approach in the literature. The 2 approaches are very similar.

For the smoothed periodogram estimator at any frequency $f$ writes: 
\begin{eqnarray}
 \label{eq:smoothedperiodogram}
  \widehat{\Gamma}_{k}=\sum_{m=0}^{2M}W_{m}P_{k-M+m}
\end{eqnarray}
where $k$ is an integer s.t. $\frac{k-1/2}{N}\leq f < \frac{k+1/2}{N}$. That leads to a mean square error (MSE) that writes
\begin{eqnarray}
\label{eq:MSEk}
 \mathrm{MSE}_{k}
 &=&
 \mathcal{G}_{k}+\mathcal{B}_{k}\mathcal{B}_{k}^{H}
\end{eqnarray}
where the bias
\begin{eqnarray}
\label{eq:biassmoothperiodogram}
 \mathcal{B}_{k}= \Gamma_{k}-\sum_{k=0}^{2M}W_{m}\Gamma_{k-M+m}
\end{eqnarray}
and the covariance
\begin{eqnarray}
\label{eq:varGammak}
 \mathcal{G}_{k}= \sum_{k=0}^{2M}W_{m}^{2}
 \diag{\Gamma_{k-M+m}}\diag{\Gamma_{k-M+m}}^{H}
\end{eqnarray}
The choice of the window and its length results from a compromise between bias and variance. Unfortunately this compromise is related to the shape of the spectrum which is \emph{unknown}. Using \eqref{eq:MSEk} we derive a confidence interval replacing deterministic values by their estimates and assuming a gaussian distribution.


%================================================
\section{More periodogram properties}
\label{ann:peridogramproperties}
%================================================
Let us consider $x_{n=0:N-1}$ a sequence of $N$ consecutive values of a real wide-sense stationary bivariate process with zero-mean and spectral matrix $\Gamma(f)$. An fundamental property says that $\Gamma(f)\geq 0$ for any value of $f$. The DFT of the sequence writes
$$
 X_{k}=\frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_{n}e^{-2j\pi kn/N}
$$
It can be shown that, for large $N$, and for $k\neq 0$ and $k\neq N/2$:
$$
 X_{k}\simiid\mathcal{N}_{c}(0,\Gamma_{k})
$$
where $\Gamma_{k}=\Gamma(k/N)$ and where $\mathcal{N}_{c}$ denotes the complex circular gaussian distribution whose the probability density writes:
$$
 p_{X_{k}}(x_{k}) = \frac{1}{\pi^{2} |\Gamma_{k}|}e^{-x_{k}^{H}\Gamma_{k}^{-1}x_{k}}
$$
Recall that for any complex gaussian random vector with zero-mean and covariance $\Gamma$ 
we have by definition:
$$
 \esp{XX^{H}} = \Gamma
$$
and
$$
 \esp{XX^{T}} = 0
$$
and if $X_{1\cdots 4}$ denote 4 gaussian complex zero-mean r.v., we have
\begin{eqnarray}
\label{eq:4moment}
\esp{X_{1}^{\beta_{1}}X_{2}^{\beta_{2}}X_{3}^{\beta_{3}}X_{4}^{\beta_{4}}}
&=&
 \esp{X_{1}^{\beta_{1}}X_{2}^{\beta_{2}}}\esp{X_{3}^{\beta_{3}}X_{4}^{\beta_{4}}}
\\
&& +\nonumber
 \esp{X_{1}^{\beta_{1}}X_{3}^{\beta_{3}}}\esp{X_{2}^{\beta_{2}}X_{4}^{\beta_{4}}}
 +
 \esp{X_{1}^{\beta_{1}}X_{4}^{\beta_{4}}}\esp{X_{2}^{\beta_{2}}X_{3}^{\beta_{3}}}
\end{eqnarray}
where $\beta_{i}$ is either ``star'' or ``not-star''.

The formula \eqref{eq:4moment} is very useful to perform the second order moments of the periodogram defined by 
\begin{eqnarray}
\label{eq:defperiodogram}
   P_{k}=X_{k}X_{k}^{H}=\begin{bmatrix}
   X_{1}X_{1}^{*}&X_{1}X_{2}^{*}
   \\
   X_{1}^{*}X_{2}&X_{2}X_{2}^{*}
   \end{bmatrix}
\end{eqnarray}
Let us remark that if we multiply $X_{1}$ and $X_{2}$ by $e^{j\theta}$ the matrix $P_{k}$ is unchanged. Then given $P_{k}$ we can determine $X_{1}$ and $X_{2}$ up to a constant of modulus 1.
It is possible to derive the distributions of $P_{k}$ entries from the distribution of $X_{k}$.
Alleviating the notations by omitting the index $k$, the distribution of these two complex r.v. $X_{1}$, and $X_{2}$ writes
\begin{eqnarray*}
 p_{X_{1},X_{2}}(x_{1},x_{2})
 &=&
 \frac{1}{\pi^{2} (\Gamma_{1,1}\Gamma_{2,2}-|\Gamma_{1,2}|^{2})}
 \\
 &&\hspace{-2cm}
 \exp\left(-
 \frac{
 |x_{1}|^{2}\Gamma_{2,2}+|x_{2}|^{2}\Gamma_{1,1}
 -2|x_{1}|\,|x_{2}||\Gamma_{2,1}|\cos(\alpha)}
 {(\Gamma_{1,1}\Gamma_{2,2}-|\Gamma_{1,2}|^{2})}
 \right)
\end{eqnarray*}
where $\alpha = \arg x_{1}-\arg x_{2}-\arg \Gamma_{1,2}$.

In practical cases we often need only the second order moments. %===========================================
\begin{example}[Variance of $P_{1,1,k}$]
Let us recall that $P_{1,1,k}\ge 0$. It comes
$$
\esp{P_{1,1,k}}=\esp{X_{1}X_{1}^{*}}=\Gamma_{1,1}\geq 0
$$
and
$$
\esp{P_{1,1,k}P_{1,1,k}^{*}}=\esp{X_{1,k}X_{1,k}^{*}X_{1,k}^{*}X_{1,k}}
=2\esp{X_{1,k}X_{1,k}^{*}}\esp{X_{1,k}X_{1,k}^{*}}+0=2\Gamma_{1,1,k}^{2}
$$
then
$$
\var{P_{1,1,k}}=\esp{P_{1,1,k}P_{1,1,k}^{*}}-\esp{P_{1,1,k}}\esp{P_{1,1,k}}^{*}
=\Gamma_{1,1,k}^{2}
$$

\end{example}
%===========================================
\begin{example}[Variance of $P_{1,2,k}$]
Consider we want to determine the second order moments of $P_{1,2,k}$ which is complex. It comes
$$
\esp{P_{1,2,k}}=\esp{X_{1,k}X_{2,k}^{*}}=\Gamma_{1,2}
$$
and
\begin{eqnarray*}
\esp{P_{1,2,k}P_{1,2,k}^{*}}&=&\esp{X_{1,k}X_{2,k}^{*}X_{1,k}^{*}X_{2,k}}
\\
 &=&
\esp{X_{1,k}X_{1,k}^{*}}\esp{X_{2,k}X_{2,k}^{*}}+
 \esp{X_{1,k}X_{2,k}^{*}}\esp{X_{2,k}X_{1,k}^{*}}+0
 \\
 &=&
 \Gamma_{1,1,k}\Gamma_{2,2,k}+\Gamma_{1,2}\Gamma_{1,2}^{*}
\end{eqnarray*}
then
$$
\var{P_{1,2,k}}=\esp{P_{1,2,k}P_{1,2,k}^{*}}-\esp{P_{1,2,k}}\esp{P_{1,2,k}^{*}}
=\Gamma_{1,1,k}\Gamma_{2,2,k}
$$
On the other hand
$$
\esp{P_{1,2,k}P_{1,2,k}} = \esp{X_{1}X_{2}^{*}X_{1}X_{2}^{*}}
=
2 \Gamma_{1,2,k}^{2}
$$
which is complex.
\end{example}

%===========================================

\begin{example}
We want to determine the variance of $P_{1,2,k,R}$. To alleviate the notations we omit the index $k$ and  let $G=P_{1,2}$, $G=P_{1,2}$ and $G_{R}=P_{1,2,R}$. Then
\begin{eqnarray*}
 G_{R}&=&\frac{1}{2}(G+G^{*})
\end{eqnarray*}
at first
$$
\esp{G_{R}}=\frac{1}{2}(\Gamma_{1,2}+\Gamma_{1,2}^{*})=\Gamma_{1,2,R}
$$
then
\begin{eqnarray*}
 \esp{G_{R}^{2}}&=& \frac{1}{4}\esp{GG+G^{*}G^{*}+2GG^{*}}
\end{eqnarray*}
at first
\begin{eqnarray*}
\esp{G^{2}} &=& \esp{X_{1}X_{2}^{*}X_{1}X_{2}^{*}}
\\
&=&\esp{X_{1}X_{2}^{*}}\esp{X_{1}X_{2}^{*}}+0+\esp{X_{1}X_{2}^{*}}\esp{X_{1}X_{2}^{*}}
\\
&=&2(\Gamma_{1,2})^{2}
\end{eqnarray*}
similarly
$$
\esp{G^{*}G^{*}}=2(\Gamma_{1,2}^{*})^{2}
$$
and
\begin{eqnarray*}
2\esp{GG^{*}} &=& 2\esp{X_{1}X_{2}^{*}X_{1}^{*}X_{2}}
\\
&=&2|\Gamma_{1,2}|^{2}+2\Gamma_{1,1}\Gamma_{2,2}+0
\end{eqnarray*}
then
\begin{eqnarray*}
 \var{G_{1,2,R}}
 &=&
 \underbrace{\frac{1}{2}(\Gamma_{1,2})^{2}+\frac{1}{2}(\Gamma_{1,2}^{*})^{2}}
 _{\Gamma_{1,2,R}^{2}-\Gamma_{1,2,I}^{2}}
 +\frac{1}{2}|\Gamma_{1,2}|^{2}+\frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}
 -\Gamma_{1,2,R}^{2}
\end{eqnarray*}
Hence
\begin{eqnarray*}
 \var{P_{1,2,k,R}}
 &=&
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}
 +\frac{1}{2}\Gamma_{1,2,R}^{2}
 -\frac{1}{2}\Gamma_{1,2,I}^{2}
\end{eqnarray*}
Similarly 
\begin{eqnarray*}
 \var{P_{1,2,k,I}}
 &=&
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}
 +\frac{1}{2}\Gamma_{1,2,I}^{2}
 -\frac{1}{2}\Gamma_{1,2,R}^{2}
\end{eqnarray*}
\end{example}



%================================================================
%================================================================
 \subsubsection{Properties of the periodogram}
 
Similarly we do in the previous examples, we have
\begin{eqnarray}
 \esp{P_{k,1,1}}= \Gamma_{k,1,1},&& 
 \var{P_{k,1,1}}= \Gamma_{k,1,1}^{2}
 \\
 \esp{P_{k,2,2}}= \Gamma_{k,2,2}, &&
 \var{P_{k,2,2}}= \Gamma_{k,2,2}^{2}
 \\
 \esp{P_{k,1,2}}= \Gamma_{k,1,2} &&
 \var{P_{k,1,2}}= \Gamma_{k,1,1}\Gamma_{k,2,2}
 \end{eqnarray}


That means that the expectation of the periodogram $P_{k}$ is $\Gamma_{k}$, that is well! But, unfortunately, the dispersion around $\Gamma_{k}$ not only never goes to 0 when $N$ goes to infinity, but it is of the order of magnitude of what we want to estimate. Consequently the periodogram is not a good estimator of $\Gamma_{k}$. 

But all good estimators of $\Gamma_{k}$ are built on the periodogram either by smoothing or by averaging. For example if we use a smoothing approach the estimator writes
\begin{eqnarray}
 \label{eq:smoothperiodogram}
 \hat \Gamma_{k} = \sum_{m=0}^{2M}W_{m}P_{k-M+m}
 %=\sum_{m=0}^{2M}W_{m}\Gamma_{k-M+m}^{1/2}Y_{k-M+m}\Gamma_{k-M+m}^{1/2}
\end{eqnarray}
with the following properties:
\begin{itemize}
\item
$\sum_{m=0}^{2M}W_{m}=1$
\item
$
 {\overline W}_{M}^{2}=\sum_{m=0}^{2M}W_{m}^{2}\rightarrow 0, \quad M \rightarrow \infty
$
\item
$M=N^{\beta}$ with $\beta\in(0,1)$, typically $\beta$ is between $0.2$ and $0.3$. That induces that both $M$ and $N/M$ go to infinity when $N$ goes to infinity.
\end{itemize}

It follows that
$$
 \esp{\hat \Gamma_{k}}=\sum_{k=0}^{2M}W_{m}\Gamma_{k-M+m}
$$
and therefore the estimator is biased with bias given by
\begin{eqnarray}
\label{eq:recallbiassmoothperiodogram}
 \mathcal{B}_{k}= \Gamma_{k}-\sum_{k=0}^{2M}W_{m}\Gamma_{k-M+m}
\end{eqnarray}
From the properties of $P_{k}$ we derive that the variance of the 4 components of $\hat \Gamma_{k}$ are given by the $4$ elements of
\begin{eqnarray*}
%\label{eq:varGammak}
 \mathcal{G}_{k}= \sum_{k=0}^{2M}W_{m}^{2}
 \diag{\Gamma_{k-M+m}}\diag{\Gamma_{k-M+m}}^{H}
\end{eqnarray*}
It results that the mean square error (MSE) writes as the $2$ by $2$ matrix
\begin{eqnarray*}
%\label{eq:MSEk}
 \mathrm{MSE}_{k}
 &=&
 \mathcal{G}_{k}+\mathcal{B}_{k}\mathcal{B}_{k}^{H}
\end{eqnarray*}
An estimate of the MSE can be obtained replacing the values of $\Gamma_{k}$ by their estimates.  Generally the variance decreases when the bias increases. Therefore the choice of a smoothing window results in a trade-off between the bias and the variance.


It is worth to noting that $\mathcal{B}_{k}$, given by \eqref{eq:recallbiassmoothperiodogram} represents the bias of the estimators. Naively you could imagine to subtract the bias to the estimator, replacing in \eqref{eq:recallbiassmoothperiodogram} the unknown values by the observed values. Of course the expectation of this quantity is 0 but for one outcome it could large and therefore by subtracting it we will re-introduce more variance.

\bigskip
It is worth to noting that the spectrum estimator variance decreases in $1/(2M+1)$ where $M$ is the length of the smoothing window for smoothed periodogram and  the number of windows for Welch's approach, but it is not related to the data number $N$ which is assumed to be \emph{infinite}. In the practical cases, we derive the value of $M$ and the window shape from a given accuracy. Then $N$ is chosen to be huge w.r.t. $M$ as for example $N=M^{3}$.



%================================================
%================================================
\section{Second order moments for the smoothed periodogram}
We omit the index $k$ and let 
\begin{eqnarray}
 \label{eq:deferiosmooth}
 \hat\Gamma = \frac{1}{2M+1}\sum_{n=0}^{2M+1}X_{n}X_{n}^{H}
 =
 \begin{bmatrix}
 \hat\Gamma_{1,1}
 &
 \hat\Gamma_{1,2}
 \\
 \hat\Gamma_{1,2}^{*}
 &
 \hat\Gamma_{2,2}
 \end{bmatrix}
 \rightarrow 
 \Gamma =
  \begin{bmatrix}
 \Gamma_{1,1}
 &
 \Gamma_{1,2}
 \\
 \Gamma_{1,2}^{*}
 &
 \Gamma_{2,2}
 \end{bmatrix}
\end{eqnarray}
where $X_{0\cdots 2M}$ are i.i.d. bivariate gaussian complex vector with zero-mean and covariance $\Gamma$. $\Gamma_{1,1}$ and $\Gamma_{2,2}$ are real. We let $\Gamma_{1,2}=\Gamma_{1,R}+j\Gamma_{1,2,I}$. 

\begin{remark}
Let us remark that the expression \eqref{eq:deferiosmooth} is as an approximation of the smoothed periodogram. Indeed (i) the window is assumed to be the rectangular window and more important (ii) the r.v. $X_{n}$ are assumed to have the same variance whereas in the equation \eqref{eq:smoothperiodogram} the r.v. have different variance, saying $\Gamma_{k}$. For non identically distributed r.v. the limit central theorem is a little bit more complicate.
\end{remark}
Here, for the two first order moments we have to perform the two first order moments of $X_{n}X_{n}^{H}$ and then divide by $1/2M+1$ for the second order moments. We have
$$
 \esp{\hat \Gamma}=\Gamma
$$
$$
 \var{\hat\Gamma_{1,1}}=\frac{1}{2M+1}\Gamma_{1,1}^{2}
$$
$$
 \var{\hat\Gamma_{2,2}}=\frac{1}{2M+1}\Gamma_{2,2}^{2}
$$
$$
 \var{\hat\Gamma_{1,2}}=\frac{1}{2M+1}\Gamma_{1,1}\Gamma_{2,2}
$$
$$
 \var{\hat\Gamma_{1,2,R}}=\frac{1}{2M+1}\frac{1}{2}
  \left( |\Gamma_{1,2}|^{2}+\Gamma_{1,1}\Gamma_{2,2}
  \right)
$$
and
$$
 \var{\hat\Gamma_{1,2,I}}=\frac{1}{2M+1}\frac{1}{2}
  \left( |\Gamma_{1,2}|^{2}+\Gamma_{1,1}\Gamma_{2,2}
  \right)
$$
It can be shown that $\Gamma_{1,2,R}$ and $\Gamma_{1,2,R}$ are correlated with
$$
 \cov{\hat\Gamma_{1,2,R},\hat\Gamma_{1,2,I}}=\Gamma_{1,2,R}\Gamma_{1,2,I}
$$


%========================================
%========================================
%========================================
\section{Approximate variances}
\label{ss:varianceexpressions}
%========================================
\subsubsection{$\delta$-method}

The so called $\delta$-method allows to derive the covariance of 
$$
 Z = f(Y)
$$
from the covariance of $Y$ where $f:\mathds{R}^{m}\mapsto \mathds{R}^{q}$. 
We let $\mu_{Y}=\esp{Y}$. Using the first order Taylor expansion of $f$ in the neighborhood  of $\mu_{Y}$, we write
$$
 Z \approx f(\mu_{Y}) + J_{\mu_{Y}}(Y-\mu_{Y})
$$
where $J_{\mu_{Y}}$ is the Jacobian of $f$, which is a matrix of size $m\times q$, performed in $y=\mu_{Y}$.
Therefore at first order
$$
 \esp{Z} \approx f(\mu_{Y}) + J_{\mu_{Y}}\esp{Y-\mu_{Y}}=f(\mu_{Y})+0
$$
then
$$
 Z-\esp{Z}\approx J_{\mu_{Y}}(Y-\mu_{Y})
$$
therefore, according to the definition $\cov{Z} = \esp{(Z-\esp{Z})(Z-\esp{Z})^{H}}$, we have
$$
 \cov{Z} \approx 
 J_{\mu_{Y}}\cov{Y}J_{\mu_{Y}}^{H}
$$

%===========================================================
\subsubsection{Approximate variance of $\arg \hat\lambda_{k}$}%\hat \phi$}

$$
 \tan(\phi) = \frac{\Gamma_{1,2,I} }{\Gamma_{1,2,R}}
$$


Then
$$
 d\phi =
 \frac{\Gamma_{1,2,R}d\Gamma_{1,2,I}-\Gamma_{1,2,I}d\Gamma_{1,2,R}}
      {|\Gamma_{1,2}|^{2}}
$$
Therefore
$$
 \var{\hat \phi}=\frac{1}{|\Gamma_{1,2}|^{4}}
 \left(
 \Gamma_{1,2,R}^{2}\var{\hat \Gamma_{1,2,I}}
 +
 \Gamma_{1,2,I}^{2}\var{\hat \Gamma_{1,2,R}}
 -
 2\Gamma_{1,2,R}\Gamma_{1,2,I}\var{\hat \Gamma_{1,2,R},\hat \Gamma_{1,2,I}}
 \right)
$$
Using that 
\begin{eqnarray*}
 \var{\hat \Gamma_{1,2,R}}
 &=&
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}+\frac{1}{2}\Gamma_{1,2,R}^{2}-\frac{1}{2}\Gamma_{1,2,I}^{2}
\end{eqnarray*}

\begin{eqnarray*}
 \var{\hat \Gamma_{1,2,I}}
 &=&
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}+\frac{1}{2}
 \Gamma_{1,2,I}^{2}-\frac{1}{2}\Gamma_{1,2,R}^{2}
\end{eqnarray*}
and
\begin{eqnarray*}
 \cov{\hat \Gamma_{1,2,R},\hat \Gamma_{1,2,I}}
 &=&
\Gamma_{1,2,R}\Gamma_{1,2,I}
\end{eqnarray*}
we get
$$
 \var{\hat \phi}=\frac{1-\mu}{2\mu}
$$
Now if we smooth on a rectangular window with weights $1/(2M+1)$ see equation \eqref{eq:rectangularsmooth}, the variance
is given by
$$
 \var{\hat \phi}=\frac{1}{2M+1}\frac{1-\mu}{2\mu}
$$
We can approximate the distribution of $\hat \phi$ by a gaussian with mean $\phi$ and variance 
$\frac{1}{2M+1}\frac{1-\mu}{2\mu}$. The green curve of the third figure of \ref{fig:statsRatiosH2Mp131} uses this approximation.
%===========================================================
\subsubsection{Approximate variance of $|\hat\lambda_{k}^{(1)}|$}%$|\hat\Gamma_{1,2}|/\Gamma_{2,2}$}
We have
$$
 R_{1}=\frac{\sqrt{\Gamma_{1,2,R}^{2}+\Gamma_{1,2,I}^{2}}}{\Gamma_{2,2}}
$$
Then
\begin{eqnarray*}
  dR_{1} &=&
 \frac{\Gamma_{1,2,R}d\Gamma_{1,2,R}+\Gamma_{1,2,I}d\Gamma_{1,2,I}}
      {\Gamma_{2,2}\sqrt{\Gamma_{1,2,R}^{2}+\Gamma_{1,2,I}^{2}}}
      -\frac{\sqrt{\Gamma_{1,2,R}^{2}+\Gamma_{1,2,I}^{2}}}{\Gamma_{2,2}^{2}}d\Gamma_{2,2}
 \\
 &=&
  \frac{\Gamma_{2,2}\Gamma_{1,2,R}d\Gamma_{1,2,R}+\Gamma_{2,2}\Gamma_{1,2,I}d\Gamma_{1,2,I}
   -|\Gamma_{1,2}|^{2}d\Gamma_{2,2}}
      {\Gamma_{2,2}^{2}|\Gamma_{1,2}|}
\end{eqnarray*}
Therefore we have to determine the expression of the variances of $\hat\Gamma_{1,2,R}$, $\hat\Gamma_{1,2,I}$, $\hat\Gamma_{2,2}$, and the 3 covariances $\cov{\hat\Gamma_{1,2,R},\hat\Gamma_{1,2,I}}$, $\cov{\hat\Gamma_{1,2,R},\hat\Gamma_{2,2}}$
and
$\cov{\hat\Gamma_{1,2,I},\hat\Gamma_{2,2}}$.

$$
 \esp{\hat\Gamma_{1,2,R},\hat\Gamma_{2,2}}
 =
 \frac{1}{2}\esp{(\hat\Gamma_{1,2}+\hat\Gamma_{1,2}^{*})\hat\Gamma_{2,2}}
 =
 \frac{1}{2}\esp{(X_{1}X_{2}^{*}+X_{1}^{*}X_{2})X_{2}X_{2}^{*}}
$$

$$
\frac{1}{2}\esp{X_{1}X_{2}^{*} X_{2}X_{2}^{*}}
+
\frac{1}{2}\esp{X_{1}^{*}X_{2} X_{2}X_{2}^{*}}
=\Gamma_{1,2}\Gamma_{2,2}+\Gamma_{1,2}^{*}\Gamma_{2,2}
=
2\Gamma_{1,2,R}\Gamma_{2,2}
$$
$$
 \cov{\hat\Gamma_{1,2,R},\hat\Gamma_{2,2}}=
 \Gamma_{1,2,R}\Gamma_{2,2}
$$
Similarly
$$
 \cov{\hat\Gamma_{1,2,I},\hat\Gamma_{2,2}}=
 \Gamma_{1,2,I}\Gamma_{2,2}
$$
Then
\begin{eqnarray}
 \var{R_{1}} = \frac{1}{\Gamma_{2,2}^{4}|\Gamma_{1,2}|^{2}}
 \left(
 A+B+C+D+E+F
 \right)
\end{eqnarray}
$$
 A =\Gamma_{2,2}^{2}\Gamma_{1,2,R}^{2}
 \left(
  \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}+\frac{1}{2}\Gamma_{1,2,R}^{2}-\frac{1}{2}\Gamma_{1,2,I}^{2}
  \right)
$$
$$
 B =\Gamma_{2,2}^{2}\Gamma_{1,2,I}^{2}
 \left(
\frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}+\frac{1}{2} \Gamma_{1,2,I}^{2}-\frac{1}{2}\Gamma_{1,2,R}^{2}
   \right)
$$
$$
 C = |\Gamma_{1,2}|^{4}\Gamma_{2,2}^{2}
$$
$$
 D = 2\Gamma_{2,2}^{2}|\Gamma_{1,2}|^{2}\Gamma_{1,2,R}\Gamma_{1,2,I}
$$
$$
 E = -2|\Gamma_{1,2}|^{2}\Gamma_{2,2}^{2}\Gamma_{1,2,R}^{2}
$$
$$
 F = -2|\Gamma_{1,2}|^{2}\Gamma_{2,2}^{2}\Gamma_{1,2,I}^{2}
$$
$$
 E+F = -2\Gamma_{2,2}^{2}|\Gamma_{1,2}|^{4}
$$
$$
 A+B = \Gamma_{2,2}^{2}
 \left(
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}|\Gamma_{1,2}|^{2}
 +
 \frac{1}{2}|\Gamma_{1,2}|^{4}
 -2\Gamma_{1,2,R}^{2}\Gamma_{1,2,I}^{2}
 \right)
$$

$$
 A+B+D+E+F = \Gamma_{2,2}^{2}
 \left(
 \frac{1}{2}\Gamma_{1,1}\Gamma_{2,2}|\Gamma_{1,2}|^{2}
 +
 \frac{1}{2}|\Gamma_{1,2}|^{4}-2|\Gamma_{1,2}|^{4}
 \right)
$$

\begin{eqnarray*}
 \var{|\Gamma_{1,2}|/\hat\Gamma_{2,2}} =\frac{1}{2}
 \left(
  \frac{\Gamma_{1,1}\Gamma_{2,2}-|\Gamma_{1,2}|^{2}}
  {\Gamma_{2,2}^{2}}
 \right)
\end{eqnarray*} 
 
%\begin{eqnarray}
% \sigma({R_{1}}) =\frac{\sqrt{\det(\Gamma)}}{\Gamma_{2,2}\sqrt{2(2M+1})}
%\end{eqnarray} 

%===========================================================
\subsubsection{Approximate variance of $|\hat\lambda_{k}^{(2)}|$}%$\hat\Gamma_{1,1}/|\Gamma_{2,1}|$}
From the previous section we have
\begin{eqnarray*}
 \var{|\hat\Gamma_{2,1}|/\hat\Gamma_{1,1}} =\frac{1}{2}
 \left(
  \frac{\Gamma_{1,1}\Gamma_{2,2}-|\Gamma_{1,2}|^{2}}
  {\Gamma_{1,1}^{2}}
 \right)
\end{eqnarray*} 
then
\begin{eqnarray*}
 \var{\hat\Gamma_{1,1}/|\hat\Gamma_{2,1}|} =\frac{1}{2}
 \left(
  (\Gamma_{1,1}\Gamma_{2,2}-|\Gamma_{1,2}|^{2})
  \frac{\Gamma_{1,1}^{2}}{|\Gamma_{1,2}|^{4}}
 \right)
\end{eqnarray*} 

%===========================================================
\subsubsection{Summary}
We can rewritten the 2 last expressions using
$$
 \mu=\frac{|\Gamma_{1,2}|^{2}}{\Gamma_{1,1}\Gamma_{2,2}}
$$
and the averaging on $(2M+1)$:
\begin{eqnarray}
\label{eq:var12on22}
 \var{|\hat\Gamma_{1,2}|/\hat\Gamma_{2,2}} =\frac{1}{2(2M+1)}
   \frac{\Gamma_{1,1}}{\Gamma_{2,2}} (1-\mu)
\end{eqnarray} 
and
\begin{eqnarray}
\label{eq:var11on21}
 \var{\hat\Gamma_{1,1}/|\hat\Gamma_{2,1}|} 
  =\frac{1}{2(2M+1)}\frac{\Gamma_{1,1}}{\Gamma_{2,2}}
  \frac{1-\mu}{\mu^{2}}
\end{eqnarray} 
Where $\mu$ is close to 1, the 2 formulas are equivalent.
%================================================
%================================================
 \subsubsection{Estimator based on eigen-decomposition (uncompleted)}
$$
 \lambda_{k}^{(3)}
 = \frac{
 \Gamma_{1,1} -\Gamma_{2,2} + ((\Gamma_{1,1} - \Gamma_{2,2})^2 + 4|\Gamma_{1,2}|^{2})^{1/2}
 }{2\Gamma_{1,2}^{*}}
$$
%==============================================================
%==============================================================
\section{Some distributions related to the smoothed periodogram}
%==============================================================
\subsubsection{Wiskart's distribution}

We consider the sequence of $2M+1$ bivariate random gaussian complex independent variables
$$
 X_{m}\simiid\mathcal{N}_{c}(0,\Gamma)
$$
We let
$$
 \Gamma =
 \begin{bmatrix}
 \sigma_{1}^{2}&\rho \sigma_{1}\sigma_{2}e^{j\theta}
 \\
 \rho \sigma_{1}\sigma_{2}e^{-j\theta}&\sigma_{2}^{2}
 \end{bmatrix}
$$
where $\sigma_{1}\geq 0$, $\sigma_{2}\geq 0$, $\rho\geq 0$ and $\theta\in(0,2\pi)$, 
and
$$
 \hat\Gamma=\frac{1}{2M+1}\sum_{m=1}^{2M+1}X_{m}X_{m}^{H}=\begin{bmatrix}
 A_{1}%\hat\Gamma_{11}
 &
 R e^{j\Phi}%\hat\Gamma_{12}
 \\
 R e^{-j\Phi}%\hat\Gamma_{12}^{*}
 &
 A_{2}%\hat\Gamma_{22}
 \end{bmatrix}
$$
where $A_{1}\geq 0$, $A_{2}\geq 0$, $R\geq 0$ and $\Phi\in(0,2\pi)$. We also have $R^{2}\leq A_{1}A_{2}$ thanks to the positivity of $\hat\Gamma$.


We let $K=2M+1$.
The distribution of $(A_{1},A_{2},R,\Phi)$ writes:
\begin{eqnarray}
\label{eq:wiskart}
 p_{A_{1}A_{2}R\Phi}
 (a_{1},a_{2},r,\phi)
 &=&
 \frac{r}{\pi (K-1)!(K-2)!|\Gamma|^{K}}
 (a_{1}a_{2}-r^{2})^{K-2}
 \\
 &&
 \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}a_{1}+\sigma_{1}^{2}a_{2}
 -2r\rho\sigma_{1}\sigma_{2}\cos(\theta-\phi)
 \right)
 \right\}
 \\
 &&
 \mathds{1}(a_{1}\geq 0)
 \times\mathds{1}(a_{2}\geq 0)
 \times\mathds{1}(\sqrt{a_{1}a_{2}}\geq r\geq 0)
 \times\mathds{1}(\phi\in(0,2\pi))
\end{eqnarray}


%==============================================================
%==============================================================
\subsubsection{Ratio distributions}
%==============================================================

We consider at first the ratio $|\hat \Gamma_{1,2}|/\hat \Gamma_{2,2}$, expression \eqref{eq:directestimateHu1}. Integrating expression \eqref{eq:wiskart} w.r.t. $\phi$ gives
\begin{eqnarray*}
p_{A_{1}A_{2}R}
 (a_{1},a_{2},r)
 &=&
 \frac{2r}{(K-1)!(K-2)!|\Gamma|^{K}}
 (a_{1}a_{2}-r^{2})^{K-2}
  \\
 &&
 \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}a_{1}+\sigma_{1}^{2}a_{2}\right)
 \right\}
 I_{0}(2r\rho\sigma_{1}\sigma_{2}/|\Gamma|)
\end{eqnarray*}
It is remarkable to notice that this expression does not depend on $\theta$. We let
\begin{eqnarray}
\left\{
 \begin{array}{rcl}
 Y_{1}&=&A_{1}
 \\
 Y_{2}&=&A_{2}
 \\
 T&=&R/A_{2}
 \end{array}
\right.
&\Leftrightarrow&
\left\{
 \begin{array}{rcl}
 A_{1}&=&Y_{1}
 \\
 A_{2}&=&Y_{2}
 \\
 R&=&TY_{2}
 \end{array}
 \right.
\end{eqnarray}
whose Jacobian is $Y_{2}$ which is positive. Because $R^{2}\leq A_{1}A_{2}$ it follows that $Y_{1}-T^{2}Y_{2}\geq 0$. Then
\begin{eqnarray*}
p_{Y_{1}Y_{2}T}
 (y_{1},y_{2},t)
 &=&
 \frac{2t y_{2}^{2}}{(K-1)!(K-2)!|\Gamma|^{K}}
 (y_{1}y_{2}-t^{2}y_{2}^{2})^{K-2}\mathds{1}(y_{1}-t^{2}y_{2}\geq 0)
  \\
 && 
 \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}y_{1}+\sigma_{1}^{2}y_{2}\right)
 \right\}
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)
\end{eqnarray*}
Integrating w.r.t. $y_{1}$ writes
\begin{eqnarray*}
p_{Y_{2}T}
 (y_{2},t)
 &=&\int_{0}^{+\infty}
 \frac{2t y_{2}^{K}}{(K-1)!(K-2)!|\Gamma|^{K}}
 (y_{1}-t^{2}y_{2})^{K-2}\mathds{1}(y_{1}-t^{2}y_{2}\geq 0)
  \\
 && 
  \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}y_{1}+\sigma_{1}^{2}y_{2}\right)
 \right\}
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)dy_{1}
\end{eqnarray*}
We let $u=y_{1}-t^{2}y_{2}$, it get
\begin{eqnarray*}
p_{Y_{2}T}
 (y_{2},t)
 &=&\int_{0}^{+\infty}
 \frac{2t y_{2}^{K}}{(K-1)!(K-2)!|\Gamma|^{K}}
 u^{K-2}
   \\
 && 
  \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}u+(t^{2}\sigma_{2}^{2}+\sigma_{1}^{2})y_{2}\right)
 \right\}
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)du
\end{eqnarray*}
and
\begin{eqnarray*}
p_{Y_{2}T}
 (y_{2},t)
 &=&\frac{2t y_{2}^{K}}{(K-1)!(K-2)!|\Gamma|^{K}}
 \,
 \exp\left\{
 -\frac{1}{|\Gamma|}(t^{2}\sigma_{2}^{2}+\sigma_{1}^{2})y_{2}\right\}
 \,
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)
 \\
 &&\int_{0}^{+\infty}
 u^{K-2} 
  \exp\left\{
 -\frac{1}{|\Gamma|} \sigma_{2}^{2}u
  \right\}du
 \end{eqnarray*}
and
\begin{eqnarray*}
p_{Y_{2}T}
 (y_{2},t)
 &=&\frac{2t y_{2}^{K}}{(K-1)!(K-2)!|\Gamma|^{K}}
 \times
 (K-2)!\frac{|\Gamma|^{K-1}}{\sigma_{2}^{2(K-1)}} 
 \\
 &&\exp\left\{
 -\frac{1}{|\Gamma|}(t^{2}\sigma_{2}^{2}+\sigma_{1}^{2})y_{2}\right\}
 \,
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)
\end{eqnarray*}

\begin{eqnarray*}
p_{Y_{2}T}
 (y_{2},t)
 &=&\frac{2t y_{2}^{K}}{(K-1)!|\Gamma|\, \sigma_{2}^{2(K-1)}}
 \,
 \exp\left\{
 -\frac{1}{|\Gamma|}(t^{2}\sigma_{2}^{2}+\sigma_{1}^{2})y_{2}\right\}
 \,
 I_{0}(2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|)
\end{eqnarray*}
We let $x=2ty_{2}\rho\sigma_{1}\sigma_{2}/|\Gamma|$ then $dx=2t\rho\sigma_{1}\sigma_{2}dy_{2}/|\Gamma|$ and
\begin{eqnarray}
 %\label{eq:RonA2distribution}
p_{T}(t)
 &=&\frac{1}{(K-1)!} \, \frac{\lambda}{\rho}
 \frac{1}{(2t)^{K}}\frac{1}{\lambda^{K}}\frac{(1-\rho^{2})^{K}}{\rho^{K}}
 \\
 && \nonumber
 \int_{0}^{+\infty}x^{K} \,
  \,
 \exp\left\{
 -\frac{1}{|\Gamma|}
 \frac{t^{2}\sigma_{2}^{2}+\sigma_{1}^{2}}{2t\rho\sigma_{1}\sigma_{2}}x\right\}
 \,
 I_{0}(x)dx
\end{eqnarray}
rewritten as
\begin{eqnarray}
 \label{eq:RonA2distribution}
p_{T}(t)
 &=&\frac{\lambda}{\rho}
 \int_{0}^{+\infty}(\zeta x)^{K} \,e^{-\xi x}I_{0}(x)dx
\end{eqnarray}
where
$$
 \zeta = \frac{1-\rho^{2}}{2t\rho\lambda}\frac{1}{((K-1)!)^{1/K}}
$$
$$
 \xi = \frac{1+t^{2}\lambda^{2}}{2t\rho\lambda}
$$
The last integration w.r.t. $x$ has been done numerically. Also we remark that
$$
 \zeta^{K} = \exp\left(
 K \log(\mu)-\sum_{k=1}^{K-1} \log(k)
 \right)
$$
where
$$
 \mu = \frac{1-\rho^{2}}{2t\rho\lambda}
$$
which is more accurate to use than factorial function. Also it is more accurate to use
$\tilde I_{0}(x)=e^{-x}I_{0}(x)$ which is usually proposed in several numerical toolboxes.


%===============================================================
We consider the ratio $\hat \Gamma_{1,1}/|\hat \Gamma_{1,2}|$, expression \eqref{eq:directestimateHu2}.

For the ratio $V=A_{1}/R$, we compute at first the distribution of $W=R/A_{2}$ which can be obtained using the expression \eqref{eq:RonA2distribution} reversing the role of $A_{1}$ and $A_{2}$. Then using the transformation $V=1/W$ whose the Jacobian is $1/V^{2}$ we get
\begin{eqnarray}
 \label{eq:A1onRdistribution}
p_{V}(v)
 &=&  \frac{\lambda}{\rho}\,v^{-2}\,
 \int_{0}^{+\infty}(\zeta x)^{K} \,e^{-\xi x}I_{0}(x)dx
\end{eqnarray}
where
$$
 \zeta = \frac{v(1-\rho^{2})}{2\rho\lambda}\frac{1}{((K-1)!)^{1/K}}
$$
$$
 \xi = \frac{v^{2}+\lambda^{2}}{2v\rho\lambda}
$$


%==============================================================
\subsubsection{Phase distribution (not completed)}
%==============================================================

We consider the sequence of $2M+1$ bivariate random gaussian complex independent variables
$$
 X_{m}\simiid\mathcal{N}_{c}(0,\Gamma)
$$
We let
$$
 \Gamma =
 \begin{bmatrix}
 \sigma_{1}^{2}&\rho \sigma_{1}\sigma_{2}e^{j\theta}
 \\
 \rho \sigma_{1}\sigma_{2}e^{-j\theta}&\sigma_{2}^{2}
 \end{bmatrix}
$$
where $\sigma_{1}\geq 0$, $\sigma_{2}\geq 0$, $\rho\geq 0$ and $\theta\in(0,2\pi)$, 
and
$$
 \hat\Gamma=\frac{1}{2M+1}\sum_{m=1}^{2M+1}X_{m}X_{m}^{H}=\begin{bmatrix}
 A_{1}%\hat\Gamma_{11}
 &
 R e^{j\Phi}%\hat\Gamma_{12}
 \\
 R e^{-j\Phi}%\hat\Gamma_{12}^{*}
 &
 A_{2}%\hat\Gamma_{22}
 \end{bmatrix}
$$
where $A_{1}\geq 0$, $A_{2}\geq 0$, $R\geq 0$ and $\Phi\in(0,2\pi)$. We also have $R^{2}\leq A_{1}A_{2}$ thanks to the positivity of $\hat\Gamma$.


We let $K=2M+1$.
The distribution of $(A_{1},A_{2},R,\Phi)$ writes:
\begin{eqnarray}
\label{eq:wiskart}
 p_{A_{1}A_{2}R\Phi}
 (a_{1},a_{2},r,\phi)
 &=& %\nonumber
 \frac{r(a_{1}a_{2}-r^{2})^{K-2}}{\pi (K-1)!(K-2)!|\Gamma|^{K}}
 \\
 && \nonumber
 \exp\left\{
 -\frac{1}{|\Gamma|}
 \left(\sigma_{2}^{2}a_{1}+\sigma_{1}^{2}a_{2}
 -2r\rho\sigma_{1}\sigma_{2}\cos(\theta-\phi)
 \right)
 \right\}
 \\
 &&\nonumber
 \mathds{1}(a_{1}\geq 0)
 \times\mathds{1}(a_{2}\geq 0)
 \times\mathds{1}(\sqrt{a_{1}a_{2}}\geq r\geq 0)
 \times\mathds{1}(\phi\in(0,2\pi))
\end{eqnarray}
We have
$$
 \int_{0}^{\sqrt{a_{1}a_{2}}} P(r)e^{-\mu r}dr = 
 \left[\frac{-1}{\mu}e^{-\mu r}\sum_{s=0}^{2K-3}\frac{1}{\mu^{s}}P^{(s)}(r)
 \right]_{0}^{\sqrt{a_{1}a_{2}}}
$$
Here $\mu = 2\rho\sigma_{1}\sigma_{2}\cos(\theta-\phi)/|\Gamma|$ and
\begin{eqnarray*}
 P(r)&=&r(a_{1}a_{2}-r^{2})^{K-2}=\sum_{q=0}^{K-2}(-1)^{q}
 (a_{1}a_{2})^{K-2-q}r^{2q+1}C_{K-2}^{q}
 \\
 P^{(1)}(r)&=&(2q+1)\sum_{q=1}^{K-2}(-1)^{q}
 (a_{1}a_{2})^{K-2-q}r^{2q}C_{K-2}^{q}
 \\
 P^{(2)}(r)&=&(2q+1)2q
 \sum_{q=1}^{K-2}(-1)^{q}
(a_{1}a_{2})^{K-2-q}r^{2q-1}C_{K-2}^{q}
 \\
 \vdots
 \\
 P^{(2K-4)}(r)&=&-(K-2)(a_{1}a_{2})^{K-3}r^{3}
 \\
 P^{(2K-3)}(r)&=&(-1)^{K-2}
\end{eqnarray*}
%For $r=0$ all derivatives are 0 except the $1$-derivative and the $(2K-3)$-derivative.
%For $r=\sqrt{a_{1}a_{2}}$ all derivatives are 0 except the $(2K-4)$-derivative and the $(2K-2)$-derivative.

\newpage
$$
 \int_{0}^{+\infty} a_{1}^{m+1/2}e^{-\alpha_{1}a_{1}}da_{1}
$$
and
$$
 \int_{0}^{+\infty} a_{1}^{m}e^{-\alpha_{1}a_{1}}da_{1}
$$
We use
$$
 \int_{0}^{+\infty} x^{t-1}e^{-\mu x}dx =\Gamma(t)\frac{1}{\mu^{t}}
$$

\dots

(uncompleted $\ldots$)





%==============================================================
%==============================================================
\section{MSC distribution}
%==============================================================

For the MSC defined in \eqref{eq:defMSC} an estimator is:
\begin{eqnarray}
\label{eq:MSCestimate}
 \hMSC_{k}= \frac{|\hat\Gamma_{k,1,2}|^{2}}{\hat\Gamma_{k,1,1}\hat\Gamma_{k,2,2}}
\end{eqnarray}
Its statistics have been given in \cite{carter:1973}. The density probability writes
\begin{eqnarray}
\label{eq:pdf-MSC}
p(c,\MSC)
 &=&\nonumber
 (N_{d}-1)\times \left[
 \frac{(1-c)(1-\MSC)}{(1-c\times\MSC)^{2}}
 \right]^{N_{d}}\times\frac{1-c\times\MSC}{(1-c)^{2}}
 \\
 &&{}_{2}F_{1}(1-N_{d},1-N_{d};1;c\times\MSC)
\end{eqnarray}
ant the cumulative function writes:
\begin{eqnarray}
\label{eq:cumulF-MSC}
F(c,\MSC)
 &=& \nonumber
\prob{\hMSC_{k}\leq c;\MSC}
\\
 &=&\nonumber
 c \times \left(
 \frac{1-\MSC}{1-c\times\MSC}
 \right)^{N_{d}} \times
  \\
 &&
 \sum_{k=0}^{N_{d}-2}\left(\frac{1-c}{1-c\times\MSC}\right)^{k}
  {}_{2}F_{1}(-k,1-N_{d};1;c\times\MSC)
\end{eqnarray}
where ${}_{2}F_{1}$ is the hypergeometric function.

The integer $N_{d}$ is either the size of the window for smoothed periodogram or the number of blocks for the averaged peridograms (Welch method). We will call it the  degree of smoothing, in short d.o.s.

Fortunately the expression \eqref{eq:cumulF-MSC} is easy to compute using that
$$
 {}_{2}F_{1}(-k,1-N_{d};1;c\times\MSC)=\sum_{m=0}^{k}T_{m}
$$ 
where $T_{0}=1$ and the recursion:
\begin{eqnarray}
\label{eq:Trecursion}
T_{m} = T_{m-1}\frac{(m-1-k)(m-N_{d})\times\MSC}{m^{2}}\,c
\end{eqnarray}
To show the accuracy of this expression we did a simple simulation. We generate a sequence of the bivariate process
$$
 x_{n} = Gw_{n}
$$
where $w_{n}$ is a bivariate process with 0 mean and covariance $I_{2}$. Therefore $x_{n}$ is a bivariate process with 0 mean  and covariance $\Gamma=GG^{H}$. Therefore the MSC of the 2 components is
$$
 \frac{|\Gamma_{1,2}|^{2}}{\Gamma_{1,1}\Gamma_{2,2}}
$$
Cumulative function has been performed from Monte-Carlo simulation and compared to the expression
\eqref{eq:cumulF-MSC}. The results are reported figure \ref{fig:cumulF-MSC}. The theoretical values are in very good agreement with those obtained from Monte-Carlo simulation. 

\figscale{cumulF-MSC.pdf}{Cumulative function of the MSC estimate given by expression \eqref{eq:MSCestimate}. The true values of the MSC are from $0.1$ to  $0.9$ by step $0.1$ from the left to the right. In blue, the values obtained by simulation on $40960$ samples. In red the theoretical expression \eqref{eq:cumulF-MSC}.}{fig:cumulF-MSC}{0.8}

\newpage
%==============================================================
\section{MSC detection}
%==============================================================

The analytical expressions of the probability density functions \eqref{eq:pdf-MSC} and the cumulative function \eqref{eq:cumulF-MSC} of the MSC estimator. Therefore we have a way to perform numerically the inverse of the cumulative function and then determine a detection threshold. Indeed, a simple dichotomy can be applied because of the monotony of the cumulative function. Then we can derive a confidence interval and attest on the MSC value.


We consider the statistical model
$$
 \{ p(\mu,\MSC_{0}), \quad \MSC_{0}\in(0,1) \}
$$
where $p$ is the probability density function of $\hMSC$ for a given value of the d.o.s.
$N_{d}$. Its expression is given in equation \eqref{eq:pdf-MSC}. We want to test the hypothesis $H_{0}=\{\MSC_{0}> C\}$ against the counter hypothesis $H_{1}=\{\MSC_{0} \leq C\}$ with $C\in(0,1)$. The GLRT writes
$$
 \Lambda(\mu) = \frac{\max_{\MSC_{0}\in(0,1)}p(\mu,\MSC_{0})}
                     {\max_{\MSC_{0}\in H_{0}}p(\mu,\MSC_{0})}
$$
The property that $\Lambda(\mu)$ is a monotonic increasing function of $c$ is not proven but we admit that $\hMSC$ is a good function of test. In this case we can fix the threshold $\eta$ such that
$$
 \prob{\hMSC\leq \eta; \MSC_{0}} = 0.95
$$
Figure \ref{fig:thresholdforC95} we have reported the $\eta$ values at $95\%$ for $\MSC_{0}\in(0,1)$ for $N_{d}=20, 30, 40, 50$. It is worth noticing that the curves is largely over the first bissectrix even for large $N_{d}$. For example for $\MSC=0.8$ the threshold is given on the zoom of the figure for different values of $N_{d}$.

\figscale{thresholdforC95.pdf}{Threshold at $95\%$ and $N_{d}=20,30,40,50$ as a function of the value on test.}{fig:thresholdforC95}{0.8}


 \newpage
 A simulation has been done with a white SOI leading to a level of MSC of $0.7$. We have reported figure \ref{fig:detectMSC} the numerical estimates of the MSC and the threshold at $95\%$. We verified that only $5\%$ are over the threshold. That means that, if we observe a value of MSC greater than around $0.8$, the probability that the MSC is under $0.7$ is less than $5\%$. That is a very conservative attitude.
 
\figscale{detectMSC.pdf}{Monte-Carlo simulations on 1000 runs. In red the true value of the MSC. In  green, the threshold derived from the analytical expression in such a way that the probability to be over this value is less than $5\%$. In black the Monte-Carlo estimates. It has been verified that $5\%$ are over the threshold.}{fig:detectMSC}{0.8}


